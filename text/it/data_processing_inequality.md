# データ処理不等式

情報理論には **データ処理不等式 (data processing inequality)** という重要な定理がある．

確率変数 $X$, $Y$, $Z$ について，次のマルコフ連鎖が成立すると仮定しよう：
$$X \rightarrow Y \rightarrow Z$$
情報理論の枠組の話なので確率変数になっているが，もっと身近に「データ」と言ってしまってよいかもしれない
（その場合，「データ」を覆う何か確率分布が存在すると思えばよい）．
この連鎖が意味するのはつまり，$X$ を処理して $Y$ を得て，その後 $X$ を**捨て**，
$Y$ のみから別の処理で $Z$ を得る，ということである．

このとき次の不等式が一般に成り立つ：
$$I(X; Z) \leq I(X; Y)$$
$I(\cdot, \cdot)$は相互情報量である．
証明は $I(X; Y, Z)$ を分解すれば簡単に得られる：
$$\begin{array}{rcl} I(X; Y, Z) & = & I(X; Z) + I(X; Y | Z) \\ & = & I(X; Y) + I(X; Z | Y) \end{array}$$
ここでマルコフ連鎖の仮定から $X$ と $Z$ は $Y$ に関して条件付独立である．したがって $I(X; Z | Y) = 0$ である．
結局，
$$I(X; Z) + I(X; Y | Z) = I(X; Y)$$
が得られるが， $I(X; Y | Z) \geq 0$ なので上記の不等式となる．
等号成立条件は $I(X; Y | Z) = 0$ であり，これは $Z$ が $X$ に関する情報を $Y$ と同程度保持していることを意味する．

この不等式は **一度データから欠落した情報を復元するのは，そのデータのみからは不可能** という重要な示唆を与える．
ここでいう欠落というのは情報を意図的に捨てる行為だけでなく， $Y$ が保持できる情報量の限界を超えていたり，
$X$ と $Y$ の表現する情報がずれていたりといった，情報の加工に起因するあらゆる影響を指す．

分かりやすい例は伝言ゲームだろう．最初の人が何か言葉を思い浮かべ ($X$)，次の人に耳打ちする．
次の人はその言葉を暗記しようとするが ($Y$)，よく聞こえなかった，全て記憶できないかったといった何らかの理由で情報が欠落する ($I(X; Y) \leq H(X)$)．
更にその次の人も同様に内容を聞くが ($Z$)，このとき当人がどれほど優秀な聴覚と記憶力を持っていても，
前の人が欠落させた情報を正しく予測することはできない（$I(X; Z) < I(X; Y)$）．
もちろん，何回かゲームを行ううちに，当てずっぽうで元の情報が偶然「復元」されることもあるだろう．
データ処理不等式はこのような単発の幸運ではなく，何度も同じ処理を繰り返した場合，それが平均的に「うまく行かない」ことを表している．
もし $Z$ が不等式を満たさない，つまり $Y$ よりも $X$ をよく反映しているとすれば，それはマルコフ連鎖の仮定が崩れていることを意味する．
たとえば3人目が1人目の喋り方をよく知っていたり，1人目の声が大きすぎて3人目にも聞こえてしまっている等， $X$ を予測するのに有用な情報が何らかの形で存在することになる．

ここに優秀な（少なくとも誰が見ても明らかなミスは生成しない）機械翻訳システムがあるとしよう．
このシステムによって生成された英語の文を用いて，英語の話者に元の日本語の意図を完全に伝えることはできるだろうか？

たとえば「青い」という単語は，日本語ではしばしば青に加えて緑まで含む概念であるが，多くの場合翻訳候補になる英語の「blue」は緑を意味しない．
つまり，「青い箱があります」と言った場合，元々の箱の色は緑だった可能性がある一方で，「There is a blue box」は箱の色が緑である事実を含まず，
翻訳という処理を行った結果，単語間の意味のずれによって処理前の情報の一部が欠落してしまうのである．



