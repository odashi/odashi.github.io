# ユニグラム言語モデル

自然言語処理では，解析対象の文書を何らかの形で確率モデル上の現象として扱う．
そのようなモデルのうち，文書に対してそれが生成される確率を与えるモデルを **言語モデル** と呼び，
他の様々なモデルの基礎となっている．

**ユニグラム (unigram) 言語モデル** は最も単純な言語モデルの一つであり，
全ての単語が独立に出現するという強い仮定の下で導出される．

今，$N$ 個の単語の列 $S := w_1 w_2 \cdots w_N$ が得られているとしよう．
これは1文でも良いし，1文書でも良いし，クローリングで得られた全ての文書を結合したものと考えても良い．
$i$ 番目の単語 $w_i$ は **語彙 (vocabulary)** $\mathcal{V}$ に含まれる記号である．
例えば英語であれば，
$S := \textrm{"this is a pen ."}$，
$w_1 = \textrm{"this"}$，
$\mathcal{V} := \left\\{ \textrm{"the"}, \textrm{"of"}, \textrm{"."}, \cdots \right\\}$
などと表すことができる．
$\mathcal{V}$ に含まれる単語の種類数は $|\mathcal{V}|$ と表すことにする．

ここで，$\mathcal{V}$ から $N$ 個の単語を復元抽出した結果 $S$ が得られたと考えよう．
このとき，$\mathcal{V}$ の各単語はどのような確率で出現すると言えるだろうか．

$S$ は求めたい確率分布に関して我々が持っている唯一の証拠なので，
あらゆる確率分布の中から $S$ に対する値が最大になるものを選ぶことにしよう．
$S$ は復元抽出で得られたので，単語 $w$ の確率を $P_w$ と書くことにすると，
$$\mathrm{Pr}(S; \Theta) := \prod_{i=1}^N P_{w_i}$$
と書けることが分かる．
ここで $\Theta$ は全ての $P_w$ の集合であり，確率分布の挙動に影響を与える **パラメータ (parameter)** である．
$S$ は同じ単語を複数回含む可能性があるので，$w$ が $S$ に出現する回数を $N_w$ と書くことにすると，
これは更に
$$\mathrm{Pr}(S; \Theta) := \prod_{w \in \mathcal{V}} P_{w_i}^{N_{w_i}}$$
と纏めることができる．
この $\mathrm{Pr}(S; \Theta)$ が最大になる $\Theta$ を求めればよいことが分かる．
このような方針のことを一般的に **最尤推定 (maximum likelihood estimation)** と呼び，
その対象となる確率分布関数（ここでは $\mathrm{Pr}(S; \Theta)$）を **尤度関数 (likelihood function)** と呼ぶ．

上記の確率そのままでは累乗が厄介なので，対数を取って式を簡単な形に変形してしまおう．
$$\log \mathrm{Pr}(S; \Theta) = \sum_{w \in \mathcal{V}} N_{w_i} \log P_{w_i}$$
対数は狭義単調増加関数なので，対数を取る前後で式の値が最大になる $\Theta$ は変化しないことに注意する．
この式を最大化すれば良いが，$P_w$ は確率なので，その総和が1になるという確率の公理を満たさなければならない．
結局，解くべき問題は次の形となる．
$$\begin{array}{ll} \mathrm{maximize} \sum_{w \in \mathcal{V}} N_{w_i} \log P_{w_i} \\\\ \mathrm{s.t.} \sum{w \in \mathcal{V}} P_w = 1 \end{array}	$$

	
